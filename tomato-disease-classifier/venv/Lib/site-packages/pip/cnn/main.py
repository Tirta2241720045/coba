from tensorflow.keras import layers, models
from pip.cnn.dataset_preparation import create_dataset
from pip.cnn.image_preprocessing import increase_brightness, filter_colors
from pip.cnn.model_architecture import create_model
from pip.cnn.training_visualization import plot_training_history


def main():
    data_dir = "/content/drive/MyDrive/KULIAH/dataset/"
    print("Creating dataset...")
    train_dataset, val_dataset = create_dataset(data_dir)

    print("Creating model...")
    model = create_model(len(train_dataset.class_names))
    print(model.summary())

    print("Training model...")
    history = train_model(model, train_dataset, val_dataset)

    print("Plotting training history...")
    plot_training_history(history)

    print("Saving model...")
    save_model(model)

    print("Processing a random image...")
    process_random_image(train_dataset)

    print("Training completed!")


def train_model(model, train_dataset, val_dataset):
    model.compile(
        optimizer="adam",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=["accuracy"],
    )

    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor="val_loss", patience=5, restore_best_weights=True
    )

    history = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=20,
        callbacks=[early_stopping],
    )

    return history


def save_model(model):
    # Simpan model ke file requirements/tomato_disease_model.h5
    requirements_dir = os.path.join(os.path.dirname(__file__), "requirements")
    os.makedirs(requirements_dir, exist_ok=True)
    model_file = os.path.join(requirements_dir, "tomato_disease_model.h5")

    # Hapus file lama jika ada, lalu simpan model yang baru
    if os.path.exists(model_file):
        os.remove(model_file)
    model.save(model_file)


def process_random_image(dataset):
    # Ambil dan proses gambar acak dari dataset
    pass


if __name__ == "__main__":
    main()
